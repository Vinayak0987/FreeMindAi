const express = require('express');
const router = express.Router();
const { body, validationResult } = require('express-validator');
const fs = require('fs');
const path = require('path');
const archiver = require('archiver');
const { v4: uuidv4 } = require('uuid');

// Mock deployment endpoints for development/testing
// In production, these would integrate with actual cloud providers

// Ensure temp directory exists
const ensureTempDir = () => {
  const tempDir = path.join(process.cwd(), 'temp');
  if (!fs.existsSync(tempDir)) {
    fs.mkdirSync(tempDir, { recursive: true });
  }
  return tempDir;
};

// Generate local project ZIP file
const generateLocalProjectZip = async (projectConfig) => {
  const { projectId, serviceName, modelConfig, taskType, dataType } = projectConfig;
  const projectName = serviceName || `ml-project-${projectId}`;
  
  // Ensure temp directory exists
  ensureTempDir();
  
  // Create temporary directory for project files
  const tempDir = path.join(process.cwd(), 'temp', projectName);
  if (!fs.existsSync(tempDir)) {
    fs.mkdirSync(tempDir, { recursive: true });
  }
  
  // Generate project files
  const files = generateProjectFiles(projectConfig);
  
  // Write files to temp directory
  for (const [filename, content] of Object.entries(files)) {
    const filePath = path.join(tempDir, filename);
    const fileDir = path.dirname(filePath);
    
    // Create directory if it doesn't exist
    if (!fs.existsSync(fileDir)) {
      fs.mkdirSync(fileDir, { recursive: true });
    }
    
    fs.writeFileSync(filePath, content);
  }
  
  // Create ZIP file
  const zipPath = path.join(process.cwd(), 'temp', `${projectName}.zip`);
  const output = fs.createWriteStream(zipPath);
  const archive = archiver('zip', { zlib: { level: 9 } });
  
  return new Promise((resolve, reject) => {
    output.on('close', () => {
      // Clean up temp directory
      fs.rmSync(tempDir, { recursive: true, force: true });
      resolve(zipPath);
    });
    
    archive.on('error', (err) => {
      reject(err);
    });
    
    archive.pipe(output);
    archive.directory(tempDir, false);
    archive.finalize();
  });
};

// Generate project files for local deployment
const generateProjectFiles = (config) => {
  const { serviceName, taskType, dataType, modelConfig, environment } = config;
  
  const files = {
    'README.md': generateReadme(config),
    'requirements.txt': generateRequirements(taskType),
    'app.py': generateFlaskApp(config),
    'model.py': generateModelCode(config),
    'config.py': generateConfig(config),
    'docker-compose.yml': generateDockerCompose(config),
    'Dockerfile': generateDockerfile(config),
    '.env.example': generateEnvExample(config),
    'run_local.py': generateRunScript(config),
    'test_api.py': generateTestScript(config)
  };
  
  return files;
};

// Generate README.md
const generateReadme = (config) => {
  const { serviceName, taskType, dataType, modelConfig } = config;
  const datasetInfo = modelConfig?.dataset ? `

## Dataset Information

- **Dataset**: ${modelConfig.dataset.name || 'Custom Dataset'}
- **Samples**: ${modelConfig.dataset.samples || 'N/A'}
- **Features**: ${modelConfig.dataset.features || 'N/A'}
- **Size**: ${modelConfig.dataset.size || 'N/A'}
` : '';
  
  return `# ${serviceName || 'ML Model'} - Local Deployment

This is a ${taskType} model for ${dataType} data, ready for local deployment.${datasetInfo}

## Quick Start

### Method 1: Python Virtual Environment
\`\`\`bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\\Scripts\\activate

# Install dependencies
pip install -r requirements.txt

# Run the application
python run_local.py
\`\`\`

### Method 2: Docker
\`\`\`bash
# Build and run with docker-compose
docker-compose up --build
\`\`\`

## API Endpoints

- **Health Check**: \`GET /health\`
- **Predict**: \`POST /predict\`
- **Model Info**: \`GET /model/info\`

## Testing

\`\`\`bash
# Test the API
python test_api.py
\`\`\`

## Configuration

Copy \`.env.example\` to \`.env\` and update the values as needed.

## Model Details

- **Task Type**: ${taskType}
- **Data Type**: ${dataType}
- **Environment**: ${config.environment || 'development'}
- **Auto-scaling**: ${config.autoScaling ? 'Enabled' : 'Disabled'}

Generated by FreeMind AI Studio - ${new Date().toISOString()}
`;
};

// Generate requirements.txt
const generateRequirements = (taskType) => {
  const baseRequirements = [
    'flask>=2.0.0',
    'numpy>=1.21.0',
    'pandas>=1.3.0',
    'scikit-learn>=1.0.0',
    'joblib>=1.0.0',
    'python-dotenv>=0.19.0',
    'flask-cors>=3.0.0'
  ];
  
  const taskSpecificRequirements = {
    'image_classification': ['tensorflow>=2.8.0', 'pillow>=8.0.0', 'opencv-python>=4.5.0'],
    'text_classification': ['transformers>=4.0.0', 'torch>=1.9.0'],
    'sentiment_analysis': ['transformers>=4.0.0', 'torch>=1.9.0'],
    'object_detection': ['tensorflow>=2.8.0', 'pillow>=8.0.0', 'opencv-python>=4.5.0'],
    'regression': ['matplotlib>=3.3.0', 'seaborn>=0.11.0'],
    'classification': ['matplotlib>=3.3.0', 'seaborn>=0.11.0']
  };
  
  const requirements = [...baseRequirements, ...(taskSpecificRequirements[taskType] || [])];
  return requirements.join('\n') + '\n';
};

// Generate Flask app
const generateFlaskApp = (config) => {
  const { serviceName, taskType } = config;
  
  return `from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import logging
from model import ModelPredictor
from config import Config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Initialize model
model_predictor = ModelPredictor()

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'service': '${serviceName || 'ml-model'}',
        'task_type': '${taskType}',
        'version': '1.0.0'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Make predictions"""
    try:
        if not request.json:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        # Get input data
        input_data = request.json.get('data')
        if input_data is None:
            return jsonify({'error': 'No data field provided'}), 400
        
        # Make prediction
        result = model_predictor.predict(input_data)
        
        return jsonify({
            'success': True,
            'prediction': result,
            'task_type': '${taskType}'
        })
        
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/model/info', methods=['GET'])
def model_info():
    """Get model information"""
    return jsonify({
        'model_name': '${serviceName || 'ml-model'}',
        'task_type': '${taskType}',
        'version': '1.0.0',
        'features': model_predictor.get_feature_info(),
        'created_at': model_predictor.get_creation_time()
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=Config.DEBUG)
`;
};

// Generate model.py
const generateModelCode = (config) => {
  const { taskType } = config;
  
  return `import numpy as np
import pandas as pd
from datetime import datetime
import joblib
import os

class ModelPredictor:
    """Model prediction class for ${taskType}"""
    
    def __init__(self):
        self.model = None
        self.feature_names = []
        self.created_at = datetime.now().isoformat()
        self.load_model()
    
    def load_model(self):
        """Load the trained model"""
        model_path = os.path.join(os.path.dirname(__file__), 'model.pkl')
        
        if os.path.exists(model_path):
            try:
                self.model = joblib.load(model_path)
                print(f"Model loaded successfully from {model_path}")
            except Exception as e:
                print(f"Error loading model: {e}")
                self._create_mock_model()
        else:
            print("No model file found, creating mock model")
            self._create_mock_model()
    
    def _create_mock_model(self):
        """Create a mock model for demonstration"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import LinearRegression
        
        # Create mock model based on task type
        if '${taskType}' in ['classification', 'image_classification', 'text_classification', 'sentiment_analysis']:
            self.model = RandomForestClassifier(n_estimators=10, random_state=42)
            # Create mock training data
            X_mock = np.random.rand(100, 5)
            y_mock = np.random.randint(0, 2, 100)
            self.model.fit(X_mock, y_mock)
            self.feature_names = [f'feature_{i}' for i in range(5)]
        else:  # regression
            self.model = LinearRegression()
            X_mock = np.random.rand(100, 5)
            y_mock = np.random.rand(100)
            self.model.fit(X_mock, y_mock)
            self.feature_names = [f'feature_{i}' for i in range(5)]
    
    def predict(self, input_data):
        """Make prediction on input data"""
        try:
            # Convert input to numpy array
            if isinstance(input_data, list):
                X = np.array(input_data).reshape(1, -1)
            elif isinstance(input_data, dict):
                # Convert dict to ordered array based on feature names
                X = np.array([[input_data.get(name, 0) for name in self.feature_names]])
            else:
                X = np.array(input_data).reshape(1, -1)
            
            # Make prediction
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(X)[0]
                prediction = self.model.predict(X)[0]
                return {
                    'prediction': int(prediction),
                    'probabilities': probabilities.tolist(),
                    'confidence': float(np.max(probabilities))
                }
            else:
                prediction = self.model.predict(X)[0]
                return {
                    'prediction': float(prediction)
                }
        except Exception as e:
            raise Exception(f"Prediction failed: {str(e)}")
    
    def get_feature_info(self):
        """Get information about model features"""
        return {
            'feature_names': self.feature_names,
            'feature_count': len(self.feature_names),
            'input_shape': f"({len(self.feature_names)},)"
        }
    
    def get_creation_time(self):
        """Get model creation timestamp"""
        return self.created_at
`;
};

// Generate config.py
const generateConfig = (config) => {
  const { environment } = config;
  
  return `import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    """Configuration class"""
    
    # Environment
    ENVIRONMENT = os.getenv('ENVIRONMENT', '${environment || 'development'}')
    DEBUG = ENVIRONMENT == 'development'
    
    # Server
    HOST = os.getenv('HOST', '0.0.0.0')
    PORT = int(os.getenv('PORT', 5000))
    
    # Model
    MODEL_PATH = os.getenv('MODEL_PATH', 'model.pkl')
    MAX_CONTENT_LENGTH = int(os.getenv('MAX_CONTENT_LENGTH', 16 * 1024 * 1024))  # 16MB
    
    # Logging
    LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
    
    # CORS
    CORS_ORIGINS = os.getenv('CORS_ORIGINS', '*')
`;
};

// Generate other files...
const generateDockerCompose = (config) => {
  const { serviceName } = config;
  
  return `version: '3.8'

services:
  ${serviceName || 'ml-model'}:
    build: .
    ports:
      - "5000:5000"
    environment:
      - ENVIRONMENT=production
      - PORT=5000
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
`;
};

const generateDockerfile = (config) => {
  return `FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]
`;
};

const generateEnvExample = (config) => {
  const { environment, serviceName } = config;
  
  return `# Environment Configuration
ENVIRONMENT=${environment || 'development'}
PORT=5000
HOST=0.0.0.0

# Model Configuration
MODEL_PATH=model.pkl
MAX_CONTENT_LENGTH=16777216

# Logging
LOG_LEVEL=INFO

# CORS
CORS_ORIGINS=*

# Service
SERVICE_NAME=${serviceName || 'ml-model'}
`;
};

const generateRunScript = (config) => {
  return `#!/usr/bin/env python3
"""Local development runner"""

import os
import sys
from app import app
from config import Config

if __name__ == '__main__':
    print(f"Starting {os.getenv('SERVICE_NAME', 'ML Model')} server...")
    print(f"Environment: {Config.ENVIRONMENT}")
    print(f"Debug mode: {Config.DEBUG}")
    print(f"Server will be available at: http://{Config.HOST}:{Config.PORT}")
    
    app.run(
        host=Config.HOST,
        port=Config.PORT,
        debug=Config.DEBUG
    )
`;
};

const generateTestScript = (config) => {
  const { taskType } = config;
  
  return `#!/usr/bin/env python3
"""API testing script"""

import requests
import json

BASE_URL = 'http://localhost:5000'

def test_health():
    """Test health endpoint"""
    response = requests.get(f'{BASE_URL}/health')
    print(f"Health Check: {response.status_code}")
    print(json.dumps(response.json(), indent=2))
    return response.status_code == 200

def test_prediction():
    """Test prediction endpoint"""
    # Sample data for testing
    test_data = {
        'data': [1.0, 2.0, 3.0, 4.0, 5.0]  # Adjust based on your model
    }
    
    response = requests.post(
        f'{BASE_URL}/predict',
        json=test_data,
        headers={'Content-Type': 'application/json'}
    )
    
    print(f"Prediction Test: {response.status_code}")
    print(json.dumps(response.json(), indent=2))
    return response.status_code == 200

def test_model_info():
    """Test model info endpoint"""
    response = requests.get(f'{BASE_URL}/model/info')
    print(f"Model Info: {response.status_code}")
    print(json.dumps(response.json(), indent=2))
    return response.status_code == 200

if __name__ == '__main__':
    print("Testing ${taskType} API...")
    print("=" * 50)
    
    try:
        health_ok = test_health()
        print("\n" + "-" * 30)
        
        info_ok = test_model_info()
        print("\n" + "-" * 30)
        
        pred_ok = test_prediction()
        print("\n" + "=" * 50)
        
        if health_ok and info_ok and pred_ok:
            print("âœ… All tests passed!")
        else:
            print("âŒ Some tests failed!")
            
    except requests.exceptions.ConnectionError:
        print("âŒ Could not connect to server. Make sure it's running on localhost:5000")
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
`;
};

// Simulate deployment process
const simulateDeployment = async (config) => {
  // Simulate processing time
  await new Promise(resolve => setTimeout(resolve, 2000));
  
  const deploymentId = `deploy_${Date.now()}`;
  const serviceName = config.serviceName || 'ml-model-service';
  
  return {
    deploymentId,
    serviceName,
    status: 'deploying',
    platform: config.platform || 'local',
    environment: config.environment || 'production',
    urls: {
      api: `https://${serviceName}.api.example.com`,
      dashboard: `https://dashboard.example.com/deployments/${deploymentId}`
    },
    startTime: new Date().toISOString(),
    estimatedCompletionTime: new Date(Date.now() + 5 * 60 * 1000).toISOString() // 5 minutes
  };
};

// Create deployment endpoint
router.post('/', [
  body('platform').optional().isString(),
  body('serviceName').optional().isString(),
  body('environment').optional().isIn(['development', 'staging', 'production']),
  body('modelConfig').optional().isObject(),
  body('deploymentConfig').optional().isObject(),
  body('taskType').optional().isString(),
  body('dataType').optional().isString()
], async (req, res) => {
  try {
    const errors = validationResult(req);
    if (!errors.isEmpty()) {
      return res.status(400).json({
        success: false,
        message: 'Validation failed',
        errors: errors.array()
      });
    }

    const { platform, serviceName, environment, modelConfig, deploymentConfig, taskType, dataType } = req.body;
    
    console.log(`ðŸš€ Starting deployment to ${platform || 'local'} platform...`);
    
    // Handle local deployment with ZIP download
    if (platform === 'local') {
      try {
        console.log('ðŸ“¦ Generating local deployment package...');
        
        // Create project configuration
        const projectConfig = {
          projectId: uuidv4(),
          serviceName: serviceName || 'ml-model',
          environment: environment || 'development',
          taskType: taskType || 'classification',
          dataType: dataType || 'tabular',
          modelConfig: modelConfig || {},
          deploymentConfig: deploymentConfig || {},
          autoScaling: deploymentConfig?.autoScaling || false
        };
        
        // Generate ZIP file
        const zipPath = await generateLocalProjectZip(projectConfig);
        const fileName = `${projectConfig.serviceName}-local-deployment.zip`;
        
        console.log('âœ… Local deployment package created successfully');
        
        // Set headers for file download
        res.setHeader('Content-Type', 'application/zip');
        res.setHeader('Content-Disposition', `attachment; filename="${fileName}"`);
        res.setHeader('Access-Control-Expose-Headers', 'Content-Disposition');
        
        // Stream the ZIP file to the response
        const fileStream = fs.createReadStream(zipPath);
        
        fileStream.on('end', () => {
          // Clean up the temporary ZIP file
          fs.unlink(zipPath, (err) => {
            if (err) {
              console.error('Error cleaning up ZIP file:', err);
            } else {
              console.log('ðŸ—‘ï¸ Cleaned up temporary ZIP file');
            }
          });
        });
        
        fileStream.on('error', (err) => {
          console.error('Error streaming ZIP file:', err);
          if (!res.headersSent) {
            res.status(500).json({
              success: false,
              message: 'Failed to download deployment package',
              error: err.message
            });
          }
        });
        
        // Pipe the file to response
        fileStream.pipe(res);
        
        return; // Don't send JSON response for file downloads
        
      } catch (zipError) {
        console.error('Error generating local deployment package:', zipError);
        return res.status(500).json({
          success: false,
          message: 'Failed to generate local deployment package',
          error: zipError.message
        });
      }
    }
    
    // Handle cloud platform deployments
    const deployment = await simulateDeployment({
      platform,
      serviceName,
      environment,
      modelConfig,
      deploymentConfig,
      taskType,
      dataType
    });
    
    res.json({
      success: true,
      message: 'Deployment started successfully',
      data: deployment
    });
    
  } catch (error) {
    console.error('Deployment error:', error);
    res.status(500).json({
      success: false,
      message: 'Deployment failed',
      error: error.message
    });
  }
});

// Get deployment status
router.get('/status/:deploymentId', async (req, res) => {
  try {
    const { deploymentId } = req.params;
    
    // Simulate status progression
    const currentTime = Date.now();
    const creationTime = parseInt(deploymentId.replace('deploy_', ''));
    const elapsedMinutes = (currentTime - creationTime) / (1000 * 60);
    
    let status, progress;
    if (elapsedMinutes < 2) {
      status = 'building';
      progress = Math.min(elapsedMinutes / 2 * 50, 50);
    } else if (elapsedMinutes < 4) {
      status = 'deploying';
      progress = 50 + Math.min((elapsedMinutes - 2) / 2 * 40, 40);
    } else if (elapsedMinutes < 5) {
      status = 'finalizing';
      progress = 90 + Math.min((elapsedMinutes - 4) / 1 * 10, 10);
    } else {
      status = 'live';
      progress = 100;
    }
    
    res.json({
      success: true,
      data: {
        deploymentId,
        status,
        progress,
        logs: [
          'ðŸ“¦ Building application...',
          'ðŸ”§ Configuring environment...',
          'ðŸš€ Deploying to cloud...',
          'âœ… Deployment complete!'
        ].slice(0, Math.ceil(progress / 25)),
        updatedAt: new Date().toISOString()
      }
    });
    
  } catch (error) {
    console.error('Deployment status error:', error);
    res.status(500).json({
      success: false,
      message: 'Failed to get deployment status',
      error: error.message
    });
  }
});

// List deployments
router.get('/', async (req, res) => {
  try {
    // Mock deployment list
    const deployments = [
      {
        deploymentId: 'deploy_1704067200000',
        serviceName: 'image-classifier',
        platform: 'vercel',
        environment: 'production',
        status: 'live',
        createdAt: '2024-01-01T00:00:00Z',
        urls: {
          api: 'https://image-classifier.api.example.com',
          dashboard: 'https://dashboard.example.com/deployments/deploy_1704067200000'
        }
      },
      {
        deploymentId: 'deploy_1704153600000',
        serviceName: 'sentiment-analyzer',
        platform: 'render',
        environment: 'staging',
        status: 'deploying',
        createdAt: '2024-01-02T00:00:00Z',
        urls: {
          api: 'https://sentiment-analyzer.api.example.com',
          dashboard: 'https://dashboard.example.com/deployments/deploy_1704153600000'
        }
      }
    ];
    
    res.json({
      success: true,
      data: deployments,
      total: deployments.length
    });
    
  } catch (error) {
    console.error('List deployments error:', error);
    res.status(500).json({
      success: false,
      message: 'Failed to get deployments',
      error: error.message
    });
  }
});

// Stop deployment
router.post('/:deploymentId/stop', async (req, res) => {
  try {
    const { deploymentId } = req.params;
    
    console.log(`ðŸ›‘ Stopping deployment: ${deploymentId}`);
    
    res.json({
      success: true,
      message: 'Deployment stopped successfully',
      data: {
        deploymentId,
        status: 'stopped',
        stoppedAt: new Date().toISOString()
      }
    });
    
  } catch (error) {
    console.error('Stop deployment error:', error);
    res.status(500).json({
      success: false,
      message: 'Failed to stop deployment',
      error: error.message
    });
  }
});

// Update deployment
router.put('/:deploymentId', [
  body('serviceName').optional().isString(),
  body('environment').optional().isIn(['development', 'staging', 'production']),
  body('modelConfig').optional().isObject()
], async (req, res) => {
  try {
    const { deploymentId } = req.params;
    const updates = req.body;
    
    console.log(`ðŸ”„ Updating deployment: ${deploymentId}`);
    
    res.json({
      success: true,
      message: 'Deployment updated successfully',
      data: {
        deploymentId,
        ...updates,
        updatedAt: new Date().toISOString()
      }
    });
    
  } catch (error) {
    console.error('Update deployment error:', error);
    res.status(500).json({
      success: false,
      message: 'Failed to update deployment',
      error: error.message
    });
  }
});

module.exports = router;
